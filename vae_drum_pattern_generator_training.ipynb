{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Current Time:     2019/03/05  11:55:32\n",
      "[info] Python Version:   3.6.5\n",
      "[info] Working Dir:      /host/home/python/musegan_npz/\n",
      "[info] GPU device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# import all library\n",
    "import librosa, IPython, datetime, time, os, sys, copy, dill, mir_eval, glob\n",
    "#import pickle\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "#from scipy.spatial.distance import euclidean, pdist, squareform\n",
    "#import IPython.display as ipd\n",
    "#from datetime import datetime\n",
    "from time import gmtime, strftime\n",
    "#from imageio import imread as imread\n",
    "#from imageio import imsave as imsave\n",
    "#import librosa.display\n",
    "import tensorflow as tf\n",
    "#from pypianoroll import Multitrack, Track\n",
    "from matplotlib import pyplot as plt\n",
    "#from midiutil.MidiFile import MIDIFile\n",
    "#import ray\n",
    "from ops import *\n",
    "#import pydub\n",
    "#from ADTLib import ADT\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# show version info\n",
    "print (\"[info] Current Time:     \" + datetime.datetime.now().strftime('%Y/%m/%d  %H:%M:%S'))\n",
    "print (\"[info] Python Version:   \" + sys.version.split('\\n')[0].split(' ')[0])\n",
    "print (\"[info] Working Dir:      \" + os.getcwd()+'/')\n",
    "\n",
    "# enable gpu usage constraint here\n",
    "limited_gpu_usage = 1     # keep \"1\" if wan to use certain GPU device\n",
    "occupied_gpu_dev = 0      # 0 for GPU 0(1080Ti), 1 for GPU 1(1080Ti), 2 for GPU 2(2080Ti)\n",
    "\n",
    "# if gpu usage is constraint, limit certain gpu for use\n",
    "if (limited_gpu_usage == 1):\n",
    "    # set available GPU\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"                       # list GPU sequence by PCI bus GPU ID\n",
    "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"                       \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(occupied_gpu_dev)   \n",
    "\n",
    "    # check available GPU\n",
    "    from tensorflow.python.client import device_lib\n",
    "    for x in range(1, len(device_lib.list_local_devices())):\n",
    "        print (\"[info] GPU \" + device_lib.list_local_devices()[x].physical_device_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read train s100 index code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] train s100 pkg is loaded.\n",
      "100\n",
      "['00063', '00516', '00550', '00607', '00626']\n",
      "100\n",
      "11303\n",
      "11303\n"
     ]
    }
   ],
   "source": [
    "# read train s100 data\n",
    "with open('./large_dataset_data/cqt_obj_pkl/song_index_s100_train_list.pkl', 'rb') as pkl_file:        \n",
    "    song_index_s100_train_list_reload = pickle.load(pkl_file)\n",
    "    \n",
    "print ('[info] train s100 pkg is loaded.')\n",
    "\n",
    "s100_song_index_list_train =     song_index_s100_train_list_reload[0]\n",
    "s100_song_bar_num_list_train =   song_index_s100_train_list_reload[1]\n",
    "s100_index_code_list_train =     song_index_s100_train_list_reload[2]\n",
    "\n",
    "print (len(s100_song_index_list_train))\n",
    "print (s100_song_index_list_train[:5])\n",
    "print (len(s100_song_bar_num_list_train))\n",
    "print (np.sum(s100_song_bar_num_list_train))\n",
    "print (len(s100_index_code_list_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read test s100 index code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] test s100 pkg is loaded.\n",
      "100\n",
      "['00244', '00353', '00367', '00395', '00680']\n",
      "100\n",
      "11338\n",
      "11338\n"
     ]
    }
   ],
   "source": [
    "# read test s100 data\n",
    "with open('./large_dataset_data/cqt_obj_pkl/song_index_s100_test_list.pkl', 'rb') as pkl_file:        \n",
    "    song_index_s100_test_list_reload = pickle.load(pkl_file)\n",
    "    \n",
    "print ('[info] test s100 pkg is loaded.')\n",
    "\n",
    "s100_song_index_list_test =     song_index_s100_test_list_reload[0]\n",
    "s100_song_bar_num_list_test =   song_index_s100_test_list_reload[1]\n",
    "s100_index_code_list_test =     song_index_s100_test_list_reload[2]\n",
    "\n",
    "print (len(s100_song_index_list_test))\n",
    "print (s100_song_index_list_test[:5])\n",
    "print (len(s100_song_bar_num_list_test))\n",
    "print (np.sum(s100_song_bar_num_list_test))\n",
    "print (len(s100_index_code_list_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show [train/test] index code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00063000sp0\n",
      "00063001sp0\n",
      "00063002sp0\n",
      "...\n",
      "00244000sp0\n",
      "00244001sp0\n",
      "00244002sp0\n"
     ]
    }
   ],
   "source": [
    "for x in s100_index_code_list_train[:3]:\n",
    "    print (x)\n",
    "print ('...')\n",
    "for x in s100_index_code_list_test[:3]:\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check style tag data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Midi obj files: 10122\n",
      "./large_dataset_data/midi_track_obj_each_songs/midi_track_obj_00000.pkl\n",
      "./large_dataset_data/midi_track_obj_each_songs/midi_track_obj_00001.pkl\n",
      "./large_dataset_data/midi_track_obj_each_songs/midi_track_obj_00002.pkl\n",
      "MIDI track object is defined.\n",
      "Reload function is defined.\n"
     ]
    }
   ],
   "source": [
    "midi_track_obj_folder = './large_dataset_data/midi_track_obj_each_songs/*.pkl'\n",
    "\n",
    "midi_track_obj_folder_flist = np.sort(glob.glob(midi_track_obj_folder, recursive=True)).tolist()\n",
    "\n",
    "print ('Midi obj files: {}'.format(len(midi_track_obj_folder_flist)))\n",
    "\n",
    "total_midi_obj_files = len(midi_track_obj_folder_flist)\n",
    "\n",
    "for x in midi_track_obj_folder_flist[:3]:\n",
    "    print (x)\n",
    "    \n",
    "\n",
    "class midi_track(object):\n",
    "    def __init__(self):\n",
    "        self.file_name = \"\"\n",
    "        self.file_msd_idx = \"\"\n",
    "        self.file_msd_id = \"\"\n",
    "        self.file_clnsd_id = \"\"\n",
    "        self.mtrack_data = []\n",
    "        self.mtrack4_data = []\n",
    "        self.pmidi_data = []\n",
    "        self.pmidi4_data_sp0 = []\n",
    "        self.pmidi4_data_sp1 = []\n",
    "        self.pmidi4_data_sp2 = []\n",
    "        self.pmidi4_data_sp3 = []\n",
    "        self.pmidi4_data_sn1 = []\n",
    "        self.pmidi4_data_sn2 = []\n",
    "        self.pmidi4_data_sn3 = []\n",
    "        self.tempo = 0        \n",
    "        self.style_tag_list = copy.deepcopy(file_unique_genre)\n",
    "        self.style_tag = \"\"\n",
    "        self.style_tag_id = 0\n",
    "        self.downbeats_list_fixed = []\n",
    "        self.bar_range_list_fixed = []\n",
    "        self.drum_bar_list = []\n",
    "        self.drum_bar_list_bin = []\n",
    "        self.drum_bar_note_num = []\n",
    "        \n",
    "print ('MIDI track object is defined.')\n",
    "\n",
    "def get_midi_object(obj_idx_in, obj_flist=midi_track_obj_folder_flist):\n",
    "    \n",
    "    file_name = obj_flist[obj_idx_in]\n",
    "    \n",
    "    with open(file_name, 'rb') as pkl_file:\n",
    "        \n",
    "        read_midi_obj = pickle.load(pkl_file)\n",
    "        \n",
    "        return (read_midi_obj)\n",
    "    \n",
    "print ('Reload function is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show ID and genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x_idx, k in enumerate(get_midi_object(0).style_tag_list):\n",
    "#print ('ID: {:2d},  name: {}'.format(x_idx, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pick a song to hack style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picked song index: 00244\n",
      "\n",
      "hacked style: \n",
      "[   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      "    0.    0.    0.  100.]\n"
     ]
    }
   ],
   "source": [
    "picked_song_id = 0\n",
    "\n",
    "picked_song_idx_str_hack = s100_song_index_list_test[picked_song_id]\n",
    "picked_song_idx_str_hack_list = [picked_song_idx_str_hack]\n",
    "print ('picked song index: {}'.format(picked_song_idx_str_hack))\n",
    "\n",
    "\n",
    "# show current style attribute\n",
    "#picked_song_idx_str_full_idx_code = picked_song_idx_str_hack + '000sp0'\n",
    "#style_id_oh = read_pkl_function(picked_song_idx_str_full_idx_code)[3]\n",
    "\n",
    "#print('current style: \\n{}'.format(style_id_oh))\n",
    "\n",
    "\n",
    "# new hacked style\n",
    "style_id_oh_hack = np.zeros(16).astype(np.float32)\n",
    "style_id_oh_hack[15] = 100.0\n",
    "\n",
    "print('\\nhacked style: \\n{}'.format(style_id_oh_hack))\n",
    "\n",
    "style_id_oh_hack_list = [style_id_oh_hack]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set add note num [ 00 / 03 / 06 / 12 / 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHKPT Ver:   v033\n",
      "Add note:    p00n\n",
      "Loss Ver:    1p2\n"
     ]
    }
   ],
   "source": [
    "chkpt_ver = 'v033'\n",
    "\n",
    "bar_add_note_num = 0\n",
    "#bar_add_note_num = 3\n",
    "#bar_add_note_num = 6\n",
    "#bar_add_note_num = 12\n",
    "#bar_add_note_num = 20\n",
    "\n",
    "rc_loss_ver_list = ['0p8', '1p0', '1p2']\n",
    "rc_loss_ver = rc_loss_ver_list[2]\n",
    "\n",
    "add_note_ver = 'p' + '{:0>2}'.format(bar_add_note_num) + 'n'\n",
    "\n",
    "print ('CHKPT Ver:   {}'.format(chkpt_ver))\n",
    "print ('Add note:    {}'.format(add_note_ver))\n",
    "print ('Loss Ver:    {}'.format(rc_loss_ver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define funtion to get data by index code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data out[0] shape: (84, 96, 8)\n",
      "data out[1] shape: (8,)\n",
      "data out[2] shape: (10,)\n",
      "data out[3] shape: (16,)\n",
      "data out[4] shape: (10,)\n",
      "data out[5] shape: (1,)\n",
      "data out[6] shape: (46, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "# read relative bar data\n",
    "with open('./large_dataset_data/cqt_obj_pkl/shcbl_short_drum_ssm.pkl', 'rb') as pkl_file:\n",
    "    hscbl_shorted = pickle.load(pkl_file)\n",
    "\n",
    "\n",
    "# define python read function\n",
    "def read_pkl_function(index_code_in):\n",
    "    \n",
    "    # convert data into correct type\n",
    "    if type(index_code_in)!=str:\n",
    "        index_code_in = index_code_in.decode(\"utf-8\")\n",
    "        \n",
    "    # extract information from index code\n",
    "    song_idx_str = index_code_in[0:5]\n",
    "    bar_idx_str = index_code_in[5:8]\n",
    "    pitch_ver_str = index_code_in[8:11]\n",
    "    \n",
    "    \n",
    "    # set parameter to get relative bars\n",
    "    get_n_rtv_bars = 8 - 1\n",
    "    rtv_bar_index = np.round(hscbl_shorted[int(song_idx_str)][int(bar_idx_str), 0, 0:get_n_rtv_bars]).astype(int)\n",
    "    rtv_bar_ratio = np.hstack([1.0, hscbl_shorted[int(song_idx_str)][int(bar_idx_str), 1, 0:get_n_rtv_bars]]).astype(np.float32)\n",
    "    \n",
    "    # save bar_name into list\n",
    "    cqt_data_fname_rtv_list = []\n",
    "    \n",
    "    #cqt_data_folder_name = './ext_dsk_nvme/large_dataset_data/cqt_obj_pkl/{}/'.format(pitch_ver_str) \n",
    "    cqt_data_folder_name = './large_dataset_data/cqt_obj_pkl/{}/'.format(pitch_ver_str) \n",
    "    cqt_data_fname = cqt_data_folder_name + index_code_in + '.pkl'\n",
    "    \n",
    "    cqt_data_fname_rtv_list.append(cqt_data_fname)\n",
    "    \n",
    "    # create read file names\n",
    "    for bar_rtv_x in rtv_bar_index:        \n",
    "        index_code_rtv = song_idx_str + '{:0>3}'.format(bar_rtv_x) + pitch_ver_str\n",
    "        cqt_data_fname_rtv = cqt_data_folder_name + index_code_rtv + '.pkl'\n",
    "        cqt_data_fname_rtv_list.append(cqt_data_fname_rtv)\n",
    "\n",
    "\n",
    "    # save data into list\n",
    "    cqt_data_rtv_bar_list = []\n",
    "    \n",
    "    # read relative files\n",
    "    for cqt_data_fname_rtv in cqt_data_fname_rtv_list:        \n",
    "        with open(cqt_data_fname_rtv, 'rb') as pkl_file:            \n",
    "            cqt_data_rtv_bar_list.append(pickle.load(pkl_file))\n",
    "            \n",
    "            \n",
    "    # load all attribute data\n",
    "    #load_atb_fname = './ext_dsk_nvme/large_dataset_data/all_merged_data_attribute_only/'\n",
    "    load_atb_fname = './large_dataset_data/all_merged_data_attribute_only/'\n",
    "    load_atb_fname += 'all_merged_data_atri_{}.pkl'.format(song_idx_str)\n",
    "    \n",
    "    with open(load_atb_fname, 'rb') as pkl_file:\n",
    "        attribute_data_list = pickle.load(pkl_file)\n",
    "        \n",
    "\n",
    "    # load drum_bar_data\n",
    "    #drum_bar_file_name = './ext_dsk_nvme/large_dataset_data/drum_arrange_46_bar_unit/'\n",
    "    drum_bar_file_name = './large_dataset_data/drum_arrange_46_bar_unit/'\n",
    "    drum_bar_file_name += 'drum_bar_{}_{}.pkl'.format(song_idx_str, bar_idx_str)\n",
    "    \n",
    "    with open(drum_bar_file_name, 'rb') as pkl_file:\n",
    "        drum_bar_data_reload = pickle.load(pkl_file)\n",
    "        \n",
    "    \n",
    "    # process CQT data\n",
    "    cqt_data_fname_rtv_mix = np.concatenate([cqt_data_rtv_bar_list[0][:,:,np.newaxis],\n",
    "                                             cqt_data_rtv_bar_list[1][:,:,np.newaxis],\n",
    "                                             cqt_data_rtv_bar_list[2][:,:,np.newaxis],\n",
    "                                             cqt_data_rtv_bar_list[3][:,:,np.newaxis],\n",
    "                                             cqt_data_rtv_bar_list[4][:,:,np.newaxis],\n",
    "                                             cqt_data_rtv_bar_list[5][:,:,np.newaxis],\n",
    "                                             cqt_data_rtv_bar_list[6][:,:,np.newaxis],\n",
    "                                             cqt_data_rtv_bar_list[7][:,:,np.newaxis]], axis=-1)\n",
    "\n",
    "    # convert data into right format\n",
    "    cqt_data_fname_rtv_mix = cqt_data_fname_rtv_mix.astype(np.float32)\n",
    "    \n",
    "\n",
    "    # process attributes\n",
    "    tempo_norm_v_oh =          attribute_data_list[1].astype(np.float32)\n",
    "    \n",
    "    \n",
    "    #style_tag_array_oh =       attribute_data_list[2].astype(np.float32)\n",
    "    if song_idx_str in picked_song_idx_str_hack_list:                        \n",
    "        style_tag_array_oh = style_id_oh_hack_list[picked_song_idx_str_hack_list.index(song_idx_str)]\n",
    "        \n",
    "    else:\n",
    "        style_tag_array_oh = attribute_data_list[2].astype(np.float32)\n",
    "    \n",
    "    \n",
    "    song_progress_oh =         attribute_data_list[3][int(bar_idx_str),:].astype(np.float32)\n",
    "    n_note_in_bar =  np.array([attribute_data_list[4][int(bar_idx_str)]]).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    # convert data into correct shape\n",
    "    drum_bar_data_reload = drum_bar_data_reload[:,:,np.newaxis].astype(np.float32)\n",
    "    \n",
    "    \n",
    "    # send back data\n",
    "    return (cqt_data_fname_rtv_mix,  \\\n",
    "            rtv_bar_ratio,           \\\n",
    "            tempo_norm_v_oh,         \\\n",
    "            style_tag_array_oh,      \\\n",
    "            song_progress_oh,        \\\n",
    "            n_note_in_bar,           \\\n",
    "            drum_bar_data_reload)\n",
    "    \n",
    "\n",
    "    \n",
    "# reading data example\n",
    "index_code_for_test = s100_index_code_list_train[1000]\n",
    "\n",
    "py_func_out_cqt_data,     \\\n",
    "py_func_out_cqt_ratio,    \\\n",
    "py_func_out_tempo_att,    \\\n",
    "py_func_out_style_att,    \\\n",
    "py_func_out_progress_att, \\\n",
    "py_func_out_n_note_att,   \\\n",
    "py_func_out_drum_arrange = read_pkl_function(index_code_for_test)\n",
    "\n",
    "# show data format\n",
    "print ('data out[0] shape: {}'.format(py_func_out_cqt_data.shape))\n",
    "print ('data out[1] shape: {}'.format(py_func_out_cqt_ratio.shape))\n",
    "print ('data out[2] shape: {}'.format(py_func_out_tempo_att.shape))\n",
    "print ('data out[3] shape: {}'.format(py_func_out_style_att.shape))\n",
    "print ('data out[4] shape: {}'.format(py_func_out_progress_att.shape))\n",
    "print ('data out[5] shape: {}'.format(py_func_out_n_note_att.shape))\n",
    "print ('data out[6] shape: {}'.format(py_func_out_drum_arrange.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define dataset.map function data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] \"dataset.map\" function is defined.\n"
     ]
    }
   ],
   "source": [
    "trf_out0_shape = py_func_out_cqt_data.shape\n",
    "trf_out1_shape = py_func_out_cqt_ratio.shape\n",
    "trf_out2_shape = py_func_out_tempo_att.shape\n",
    "trf_out3_shape = py_func_out_style_att.shape\n",
    "trf_out4_shape = py_func_out_progress_att.shape\n",
    "trf_out5_shape = py_func_out_n_note_att.shape\n",
    "trf_out6_shape = py_func_out_drum_arrange.shape\n",
    "\n",
    "def tf_reshape_function(trf_out0, trf_out1, trf_out2, trf_out3, trf_out4, trf_out5, trf_out6):\n",
    "    \n",
    "    trf_out0.set_shape(trf_out0_shape)    # cqt data\n",
    "    trf_out1.set_shape(trf_out1_shape)    # cqt data ratio\n",
    "    trf_out2.set_shape(trf_out2_shape)    # tempo data\n",
    "    trf_out3.set_shape(trf_out3_shape)    # style data\n",
    "    trf_out4.set_shape(trf_out4_shape)    # song progress\n",
    "    trf_out5.set_shape(trf_out5_shape)    # note number in bar\n",
    "    trf_out6.set_shape(trf_out6_shape)    # drum arrange    \n",
    "    \n",
    "    return trf_out0, trf_out1, trf_out2, trf_out3, trf_out4, trf_out5, trf_out6\n",
    "\n",
    "print('[info] \\\"dataset.map\\\" function is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define TF dataset API for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Total train index codes: 11303\n",
      "[info] TF train Data API is defined.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 * 1\n",
    "\n",
    "print ('[info] Total train index codes: {}'.format(len(s100_index_code_list_train)))\n",
    "\n",
    "darr_train_dataset = tf.data.Dataset.from_tensor_slices((s100_index_code_list_train))\n",
    "darr_train_dataset = darr_train_dataset.map(lambda index_code_train: tuple(tf.py_func(read_pkl_function,                    \n",
    "                                                                                      [index_code_train],\n",
    "                                                                                      [tf.float32, \n",
    "                                                                                       tf.float32, \n",
    "                                                                                       tf.float32, \n",
    "                                                                                       tf.float32, \n",
    "                                                                                       tf.float32, \n",
    "                                                                                       tf.float32, \n",
    "                                                                                       tf.float32])),\n",
    "                                            num_parallel_calls=32)\n",
    "\n",
    "darr_train_dataset = darr_train_dataset.map(tf_reshape_function, num_parallel_calls=32)\n",
    "darr_train_dataset = darr_train_dataset.batch(batch_size=batch_size)\n",
    "\n",
    "train_iter = darr_train_dataset.make_initializable_iterator()\n",
    "\n",
    "# get batch data\n",
    "batch_bar_cqt_data_train,         \\\n",
    "batch_bar_cqt_ratio_train,        \\\n",
    "batch_bar_tempo_data_train,       \\\n",
    "batch_bar_style_data_train,       \\\n",
    "batch_bar_progress_train,         \\\n",
    "batch_bar_note_num_train,         \\\n",
    "batch_bar_arrange_train = train_iter.get_next()\n",
    "\n",
    "print('[info] TF train Data API is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define TF dataset API for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Total test index codes: 11338\n",
      "[info] TF test Data API is defined.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 * 1\n",
    "\n",
    "print ('[info] Total test index codes: {}'.format(len(s100_index_code_list_test)))\n",
    "\n",
    "darr_test_dataset = tf.data.Dataset.from_tensor_slices((s100_index_code_list_test))\n",
    "darr_test_dataset = darr_test_dataset.map(lambda index_code_test: tuple(tf.py_func(read_pkl_function,                    \n",
    "                                                                                   [index_code_test],\n",
    "                                                                                   [tf.float32, \n",
    "                                                                                    tf.float32, \n",
    "                                                                                    tf.float32, \n",
    "                                                                                    tf.float32, \n",
    "                                                                                    tf.float32, \n",
    "                                                                                    tf.float32, \n",
    "                                                                                    tf.float32])),\n",
    "                                           num_parallel_calls=32)\n",
    "\n",
    "darr_test_dataset = darr_test_dataset.map(tf_reshape_function, num_parallel_calls=16)\n",
    "darr_test_dataset = darr_test_dataset.batch(batch_size=batch_size)\n",
    "\n",
    "test_iter = darr_test_dataset.make_initializable_iterator()\n",
    "\n",
    "# get batch data\n",
    "batch_bar_cqt_data_test,         \\\n",
    "batch_bar_cqt_ratio_test,        \\\n",
    "batch_bar_tempo_data_test,       \\\n",
    "batch_bar_style_data_test,       \\\n",
    "batch_bar_progress_test,         \\\n",
    "batch_bar_note_num_test,         \\\n",
    "batch_bar_arrange_test = test_iter.get_next()\n",
    "\n",
    "print('[info] TF test Data API is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder define done.\n"
     ]
    }
   ],
   "source": [
    "# define leaky relu function\n",
    "def lrelu(x, alpha=0.05):\n",
    "    return tf.maximum(x, tf.multiply(x, alpha))\n",
    "\n",
    "n_latent = 32\n",
    "\n",
    "# define spectrogram encoder\n",
    "def spec_encoder(enc_song_tempo,        # (batch_num, 10)\n",
    "                 enc_style_id,          # (batch_num, 15)\n",
    "                 enc_song_progress,     # (batch_num, 10)  \n",
    "                 enc_spectrogram,       # (batch_num, 84, 96, 2)\n",
    "                 reuse=False):\n",
    "    \n",
    "    with tf.variable_scope('spec_nn_enc', reuse=reuse):\n",
    "        \n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            \n",
    "        else:\n",
    "            assert tf.get_variable_scope().reuse is False    \n",
    "        \n",
    "        # define song_tempo input layer\n",
    "        enc_song_tempo_i_layer = tf.layers.dense(inputs=enc_song_tempo,\n",
    "                                                 units=64,\n",
    "                                                 activation=lrelu,\n",
    "                                                 name='enc_nn_at1')                                                    \n",
    "\n",
    "        # define style_id input layer\n",
    "        enc_style_id_i_layer = tf.layers.dense(inputs=enc_style_id,\n",
    "                                               units=64,\n",
    "                                               activation=lrelu,\n",
    "                                               name='enc_nn_at2')          \n",
    "        # define song_progress input layer\n",
    "        enc_song_progress_i_layer = tf.layers.dense(inputs=enc_song_progress,\n",
    "                                                    units=64,\n",
    "                                                    activation=lrelu,\n",
    "                                                    name='enc_nn_at3')\n",
    "        \n",
    "        # make padding Batch / Height / Width / Channel \n",
    "        enc_spectrogram_pad = tf.pad(enc_spectrogram, [[0, 0], [1, 1], [1, 1], [0, 0]], \"CONSTANT\")\n",
    "                \n",
    "        enc_conv_h1 = tf.nn.elu(instance_norm(conv2d(enc_spectrogram_pad, \n",
    "                                                     output_dim=48,\n",
    "                                                     ks=[4,4],\n",
    "                                                     s=[1,1], \n",
    "                                                     name='enc_conv1'), 'enc_bn1'))\n",
    "    \n",
    "        enc_conv_h2 = tf.nn.elu(instance_norm(conv2d(enc_conv_h1, \n",
    "                                                     output_dim=48,\n",
    "                                                     ks=[4,4],\n",
    "                                                     s=[2,2], \n",
    "                                                     name='enc_conv2'), 'enc_bn2'))\n",
    "        \n",
    "        enc_conv_h3 = tf.nn.elu(instance_norm(conv2d(enc_conv_h2,\n",
    "                                                     output_dim=72,\n",
    "                                                     ks=[4,4],\n",
    "                                                     s=[2,2], \n",
    "                                                     name='enc_conv3'), 'enc_bn3'))\n",
    "    \n",
    "    \n",
    "        # flatten conv output\n",
    "        enc_convo_flat_out = tf.reshape(enc_conv_h3, [-1, np.prod(enc_conv_h3.get_shape()[1:])])\n",
    "        \n",
    "        # concat all decoder input layers\n",
    "        enc_merged_layer = tf.concat([enc_song_tempo_i_layer,       \\\n",
    "                                      enc_style_id_i_layer,         \\\n",
    "                                      enc_song_progress_i_layer,    \\\n",
    "                                      enc_convo_flat_out],          \\\n",
    "                                     axis=1,                        \\\n",
    "                                     name='enc_nn_in_concat')\n",
    "    \n",
    "        \n",
    "        enc_mlp_h1 = tf.layers.dense(inputs=enc_merged_layer,\n",
    "                                 units=1024,\n",
    "                                 activation=lrelu,\n",
    "                                 name='enc_nn_mid_h1')\n",
    "        \n",
    "        enc_mlp_h2 = tf.layers.dense(inputs=enc_mlp_h1,\n",
    "                                 units=1024,\n",
    "                                 activation=lrelu,\n",
    "                                 name='enc_nn_mid_h2')\n",
    "\n",
    "        enc_mlp_h2m = lrelu(enc_mlp_h2 + enc_mlp_h1*0.2)\n",
    "        \n",
    "        enc_mlp_h3 = tf.layers.dense(inputs=enc_mlp_h2m,\n",
    "                                     units=1024,\n",
    "                                     activation=lrelu,\n",
    "                                     name='enc_nn_mid_h3')\n",
    "        \n",
    "        enc_mlp_h3m = lrelu(enc_mlp_h3 + enc_mlp_h2m*0.2)      \n",
    "        \n",
    "        \n",
    "        # define encoder output layer\n",
    "        z_mean = tf.layers.dense(inputs=enc_mlp_h3m,\n",
    "                                 units=n_latent,\n",
    "                                 activation=None,\n",
    "                                 name='enco_mean')\n",
    "            \n",
    "        z_std = tf.layers.dense(inputs=enc_mlp_h3m, \n",
    "                                units=n_latent, \n",
    "                                activation=None,\n",
    "                                name='enco_std')\n",
    "        \n",
    "        z_epsilon = tf.random_normal(tf.stack([tf.shape(enc_mlp_h3m)[0], n_latent])) \n",
    "        \n",
    "        z_latent  = z_mean + tf.multiply(z_epsilon, tf.exp(z_std * 0.5))\n",
    "        \n",
    "        enc_n_note_h1 = tf.layers.dense(inputs=enc_mlp_h3m, \n",
    "                                        units=512, \n",
    "                                        activation=lrelu,\n",
    "                                        name='enc_nnp_h1')\n",
    "\n",
    "        enc_n_note_h2 = tf.layers.dense(inputs=enc_n_note_h1, \n",
    "                                        units=512, \n",
    "                                        activation=lrelu,\n",
    "                                        name='enc_nnp_h2')        \n",
    "        \n",
    "        enc_n_note_pridiction = tf.layers.dense(inputs=enc_n_note_h2, \n",
    "                                                units=1, \n",
    "                                                activation=tf.nn.sigmoid,\n",
    "                                                name='enco_nnp_out')\n",
    "        \n",
    "        enc_n_note_pridiction_x256 = (enc_n_note_pridiction * 256) - 10\n",
    "        \n",
    "        return z_latent, z_mean, z_std, enc_n_note_pridiction_x256\n",
    "    \n",
    "print ('Encoder define done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder define done.\n"
     ]
    }
   ],
   "source": [
    "dec_output_size = np.prod(trf_out6_shape)   # bar_arrange, out[5] shape: (46, 16, 1)\n",
    "dec_output_size_4d = [-1, trf_out6_shape[0], trf_out6_shape[1], 1]\n",
    "\n",
    "# define leaky relu function\n",
    "def lrelu(x, alpha=0.05):\n",
    "    return tf.maximum(x, tf.multiply(x, alpha))\n",
    "\n",
    "# define spectrogram encoder\n",
    "def spec_decoder(dec_song_tempo,        # (batch_num, 10)\n",
    "                 dec_style_id,          # (batch_num, 15)\n",
    "                 dec_song_progress,     # (batch_num, 10)   \n",
    "                 dec_bar_note_num,      # (batch_num, 1)\n",
    "                 dec_z_sampled,         # (batch_num, 32)                 \n",
    "                 reuse=False):\n",
    "    \n",
    "    with tf.variable_scope('spec_nn_dec', reuse=reuse):\n",
    "        \n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            \n",
    "        else:\n",
    "            assert tf.get_variable_scope().reuse is False          \n",
    "            \n",
    "        # define song_tempo input layer\n",
    "        dec_song_tempo_i_layer = tf.layers.dense(inputs=dec_song_tempo,\n",
    "                                                 units=64,\n",
    "                                                 activation=lrelu,\n",
    "                                                 name='dec_nn_at1')                                                    \n",
    "\n",
    "        # define style_id input layer\n",
    "        dec_style_id_i_layer = tf.layers.dense(inputs=dec_style_id,\n",
    "                                               units=64,\n",
    "                                               activation=lrelu,\n",
    "                                               name='dec_nn_at2')          \n",
    "        # define song_progress input layer\n",
    "        dec_song_progress_i_layer = tf.layers.dense(inputs=dec_song_progress,\n",
    "                                                    units=64,\n",
    "                                                    activation=lrelu,\n",
    "                                                    name='dec_nn_at3')\n",
    "        \n",
    "        # define bar_note_num input layer\n",
    "        dec_bar_note_num_limited = tf.clip_by_value(dec_bar_note_num,\n",
    "                                                    0.0,\n",
    "                                                    200.0)\n",
    "        \n",
    "        dec_bar_note_num_i_layer = tf.layers.dense(inputs=dec_bar_note_num_limited,\n",
    "                                                   units=64,\n",
    "                                                   activation=lrelu,\n",
    "                                                   name='dec_nn_at4')\n",
    "        \n",
    "        # define z input layer\n",
    "        dec_z_i_layer = tf.layers.dense(inputs=dec_z_sampled,\n",
    "                                        units=256,\n",
    "                                        activation=lrelu,\n",
    "                                        name='dec_nn_at5')\n",
    "        \n",
    "        # concat all decoder input layers\n",
    "        dec_merged_layer = tf.concat([dec_song_tempo_i_layer,       \\\n",
    "                                      dec_style_id_i_layer,         \\\n",
    "                                      dec_song_progress_i_layer,    \\\n",
    "                                      dec_bar_note_num_i_layer,     \\\n",
    "                                      dec_z_i_layer],               \\\n",
    "                                     axis=1,                        \\\n",
    "                                     name='dec_nn_in_concat')\n",
    "                \n",
    "        dec_mlp_h1 = tf.layers.dense(inputs=dec_merged_layer,\n",
    "                                     units=1024,\n",
    "                                     activation=lrelu,\n",
    "                                     name='dec_nn_mid_h1')                                     \n",
    "        \n",
    "        dec_mlp_h2 = tf.layers.dense(inputs=dec_mlp_h1,\n",
    "                                     units=1024,\n",
    "                                     activation=lrelu,\n",
    "                                     name='dec_nn_mid_h2')   \n",
    "        \n",
    "        dec_mlp_h2m = lrelu(dec_mlp_h2 + dec_mlp_h1*0.2)\n",
    "        \n",
    "        dec_mlp_h3 = tf.layers.dense(inputs=dec_mlp_h2m,\n",
    "                                     units=2048,\n",
    "                                     activation=lrelu,\n",
    "                                     name='dec_nn_mid_h3')\n",
    "        \n",
    "        dec_mlp_h4 = tf.layers.dense(inputs=dec_mlp_h3,\n",
    "                                     units=2048,\n",
    "                                     activation=lrelu,\n",
    "                                     name='dec_nn_mid_h4')\n",
    "        \n",
    "        dec_mlp_h4m = lrelu(dec_mlp_h4 + dec_mlp_h3*0.2)\n",
    "        \n",
    "        dec_mlp_h5 = tf.layers.dense(inputs=dec_mlp_h4m,\n",
    "                                     units=2048,\n",
    "                                     activation=lrelu,\n",
    "                                     name='dec_nn_mid_h5')\n",
    "        \n",
    "        dec_mlp_h5m = lrelu(dec_mlp_h5 + dec_mlp_h4m*0.2)\n",
    "        \n",
    "        dec_mlp_h6 = tf.layers.dense(inputs=dec_mlp_h5m,\n",
    "                                     units=2048,\n",
    "                                     activation=lrelu,\n",
    "                                     name='dec_nn_mid_h6')\n",
    "        \n",
    "        dec_mlp_h6m = lrelu(dec_mlp_h6 + dec_mlp_h5m*0.2)\n",
    "        \n",
    "        # final output layer use tanh\n",
    "        dec_mlp_output = tf.layers.dense(inputs=dec_mlp_h6m,\n",
    "                                         units=dec_output_size,                        \n",
    "                                         activation=tf.nn.tanh,\n",
    "                                         name='dec_nn_out_final')        \n",
    "        \n",
    "        # normalize output range to -1.5 ~ 2.5\n",
    "        #dec_mlp_output_norm = (dec_mlp_output * 4.0) + 0.5        \n",
    "        dec_mlp_output_norm = lrelu((dec_mlp_output * 2.0) + 0.5)\n",
    "        \n",
    "        # reshape data into 4d shape\n",
    "        dec_output_reshape = tf.reshape(dec_mlp_output_norm, dec_output_size_4d)\n",
    "        \n",
    "        return dec_output_reshape\n",
    "\n",
    "print ('Decoder define done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define function to apply cqt ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_broadcast(tensor, shape):\n",
    "    \n",
    "    return (tensor + tf.zeros(shape, dtype=tensor.dtype))\n",
    "\n",
    "def apply_cqt_ratio(cqt_tensor_in, ratio_tensor_in):\n",
    "    \n",
    "    ratio_tensor_in_4d = tf.reshape(ratio_tensor_in, tf.shape(cqt_tensor_in[:,0:1,0:1,:]))\n",
    "    \n",
    "    ratio_tensor_in_expanded = tf_broadcast(ratio_tensor_in_4d, tf.shape(cqt_tensor_in[:,:,:,:]))\n",
    "    \n",
    "    return (tf.multiply(cqt_tensor_in, ratio_tensor_in_expanded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define function to calculate diff y layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff-2 function define done.\n"
     ]
    }
   ],
   "source": [
    "# note: input tensor must be 4-D data\n",
    "def get_matx_2_layer_tf(matx_data_in):\n",
    "    \n",
    "    matx_layer_0 = matx_data_in\n",
    "    \n",
    "    matx_data_pady = tf.pad(matx_data_in,\n",
    "                            paddings=[[0,0], [0,0], [1,0], [0,0]],\n",
    "                            mode='CONSTANT',\n",
    "                            name='tf_diff2_pady',\n",
    "                            constant_values=0\n",
    "                            )[:, :, :-1, :]\n",
    "                            #)[:, :matx_data_in.get_shape()[1], :, :]\n",
    "        \n",
    "    matx_layer_1 = matx_data_in - matx_data_pady\n",
    "    \n",
    "    matx_layer_1_concat = tf.concat([tf.zeros_like(matx_layer_1)[:,:,0:1,:], \n",
    "                                     matx_layer_1[:,:,1:,:]],\n",
    "                                    axis=2)\n",
    "    \n",
    "    \n",
    "    matx_layer_all = tf.concat([matx_layer_0, matx_layer_1_concat], \n",
    "                               axis=-1,\n",
    "                               name='tf_diff2_concat')\n",
    "    \n",
    "    return matx_layer_all    \n",
    "    \n",
    "print ('Diff-2 function define done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define function to calculate 3 layer (add diif x/y layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff-3 function define done.\n"
     ]
    }
   ],
   "source": [
    "# note: input tensor must be 4-D data\n",
    "def get_matx_3_layer_tf(matx_data_in):\n",
    "    \n",
    "    matx_layer_0 = matx_data_in\n",
    "    \n",
    "    matx_data_pady = tf.pad(matx_data_in,\n",
    "                            paddings=[[0,0], [0,0], [1,0], [0,0]],\n",
    "                            mode='CONSTANT',\n",
    "                            name='tf_diff3_pady',\n",
    "                            constant_values=0\n",
    "                            )[:, :, :-1, :]\n",
    "                            #)[:, :matx_data_in.get_shape()[1], :, :]\n",
    "        \n",
    "    matx_layer_1 = matx_data_in - matx_data_pady\n",
    "    \n",
    "    matx_layer_1_concat = tf.concat([tf.zeros_like(matx_layer_1)[:,:,0:1,:], \n",
    "                                     matx_layer_1[:,:,1:,:]],\n",
    "                                    axis=2)\n",
    "    \n",
    "    matx_data_padx = tf.pad(matx_data_in,\n",
    "                            paddings=[[0,0], [1,0], [0,0], [0,0]],\n",
    "                            mode='CONSTANT',\n",
    "                            name='tf_diff3_padx',\n",
    "                            constant_values=0\n",
    "                            )[:, :-1, :, :]\n",
    "                            #)[:, :, :matx_data_in.get_shape()[2], :]    \n",
    "    \n",
    "    matx_layer_2 = matx_data_in - matx_data_padx\n",
    "    \n",
    "    matx_layer_2_concat = tf.concat([tf.zeros_like(matx_layer_1)[:,0:1,:,:], \n",
    "                                     matx_layer_1[:,1:,:,:]],\n",
    "                                    axis=1)\n",
    "    \n",
    "    matx_layer_all = tf.concat([matx_layer_0, matx_layer_1_concat, matx_layer_2_concat], \n",
    "                               axis=-1,\n",
    "                               name='tf_diff3_concat')\n",
    "    \n",
    "    return matx_layer_all    \n",
    "    \n",
    "print ('Diff-3 function define done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Model Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] VAE train model is connected.\n",
      "[info] VAE test model is connected.\n"
     ]
    }
   ],
   "source": [
    "# connect model for training data\n",
    "processed_cqt_data_train = apply_cqt_ratio(batch_bar_cqt_data_train, batch_bar_cqt_ratio_train)\n",
    "processed_cqt_data_double_layer_train = get_matx_2_layer_tf(processed_cqt_data_train)\n",
    "\n",
    "vae_latent_z_train,                         \\\n",
    "vae_latent_zmn_train,                       \\\n",
    "vae_latent_zsd_train,                       \\\n",
    "vae_note_pred_train = spec_encoder(batch_bar_tempo_data_train,                 \\\n",
    "                                   batch_bar_style_data_train,                 \\\n",
    "                                   batch_bar_progress_train,                   \\\n",
    "                                   processed_cqt_data_double_layer_train,      \\\n",
    "                                   reuse=False)\n",
    "\n",
    "vae_drum_out_train = spec_decoder(batch_bar_tempo_data_train,                  \\\n",
    "                                  batch_bar_style_data_train,                  \\\n",
    "                                  batch_bar_progress_train,                    \\\n",
    "                                  vae_note_pred_train,                         \\\n",
    "                                  vae_latent_z_train,                          \\\n",
    "                                  reuse=False)\n",
    "\n",
    "print ('[info] VAE train model is connected.')\n",
    "\n",
    "\n",
    "\n",
    "# connect model for testing data\n",
    "processed_cqt_data_test = apply_cqt_ratio(batch_bar_cqt_data_test, batch_bar_cqt_ratio_test)\n",
    "processed_cqt_data_double_layer_test = get_matx_2_layer_tf(processed_cqt_data_test)\n",
    "\n",
    "vae_latent_z_test,     \\\n",
    "vae_latent_zmn_test,   \\\n",
    "vae_latent_zsd_test,   \\\n",
    "vae_note_pred_test = spec_encoder(batch_bar_tempo_data_test,                   \\\n",
    "                                  batch_bar_style_data_test,                   \\\n",
    "                                  batch_bar_progress_test,                     \\\n",
    "                                  processed_cqt_data_double_layer_test,        \\\n",
    "                                  reuse=tf.AUTO_REUSE)\n",
    "\n",
    "vae_drum_out_test = spec_decoder(batch_bar_tempo_data_test,                    \\\n",
    "                                 batch_bar_style_data_test,                    \\\n",
    "                                 batch_bar_progress_test,                      \\\n",
    "                                 vae_note_pred_test + bar_add_note_num,        \\\n",
    "                                 vae_latent_z_test,                            \\\n",
    "                                 reuse=tf.AUTO_REUSE)\n",
    "\n",
    "print ('[info] VAE test model is connected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define all trainable variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Total params: 61590385\n",
      "[info] Encoder params: 43808081\n",
      "[info] Decoder params: 17782304\n",
      "\n",
      "['spec_nn_enc/enc_nn_at1/kernel:0', 'spec_nn_enc/enc_nn_at1/bias:0', 'spec_nn_enc/enc_nn_at2/kernel:0', 'spec_nn_enc/enc_nn_at2/bias:0', 'spec_nn_enc/enc_nn_at3/kernel:0', 'spec_nn_enc/enc_nn_at3/bias:0', 'spec_nn_enc/enc_conv1/Conv/weights:0', 'spec_nn_enc/enc_bn1/scale:0', 'spec_nn_enc/enc_bn1/offset:0', 'spec_nn_enc/enc_conv2/Conv/weights:0', 'spec_nn_enc/enc_bn2/scale:0', 'spec_nn_enc/enc_bn2/offset:0', 'spec_nn_enc/enc_conv3/Conv/weights:0', 'spec_nn_enc/enc_bn3/scale:0', 'spec_nn_enc/enc_bn3/offset:0', 'spec_nn_enc/enc_nn_mid_h1/kernel:0', 'spec_nn_enc/enc_nn_mid_h1/bias:0', 'spec_nn_enc/enc_nn_mid_h2/kernel:0', 'spec_nn_enc/enc_nn_mid_h2/bias:0', 'spec_nn_enc/enc_nn_mid_h3/kernel:0', 'spec_nn_enc/enc_nn_mid_h3/bias:0', 'spec_nn_enc/enco_mean/kernel:0', 'spec_nn_enc/enco_mean/bias:0', 'spec_nn_enc/enco_std/kernel:0', 'spec_nn_enc/enco_std/bias:0', 'spec_nn_enc/enc_nnp_h1/kernel:0', 'spec_nn_enc/enc_nnp_h1/bias:0', 'spec_nn_enc/enc_nnp_h2/kernel:0', 'spec_nn_enc/enc_nnp_h2/bias:0', 'spec_nn_enc/enco_nnp_out/kernel:0', 'spec_nn_enc/enco_nnp_out/bias:0', 'spec_nn_dec/dec_nn_at1/kernel:0', 'spec_nn_dec/dec_nn_at1/bias:0', 'spec_nn_dec/dec_nn_at2/kernel:0', 'spec_nn_dec/dec_nn_at2/bias:0', 'spec_nn_dec/dec_nn_at3/kernel:0', 'spec_nn_dec/dec_nn_at3/bias:0', 'spec_nn_dec/dec_nn_at4/kernel:0', 'spec_nn_dec/dec_nn_at4/bias:0', 'spec_nn_dec/dec_nn_at5/kernel:0', 'spec_nn_dec/dec_nn_at5/bias:0', 'spec_nn_dec/dec_nn_mid_h1/kernel:0', 'spec_nn_dec/dec_nn_mid_h1/bias:0', 'spec_nn_dec/dec_nn_mid_h2/kernel:0', 'spec_nn_dec/dec_nn_mid_h2/bias:0', 'spec_nn_dec/dec_nn_mid_h3/kernel:0', 'spec_nn_dec/dec_nn_mid_h3/bias:0', 'spec_nn_dec/dec_nn_mid_h4/kernel:0', 'spec_nn_dec/dec_nn_mid_h4/bias:0', 'spec_nn_dec/dec_nn_mid_h5/kernel:0', 'spec_nn_dec/dec_nn_mid_h5/bias:0', 'spec_nn_dec/dec_nn_mid_h6/kernel:0', 'spec_nn_dec/dec_nn_mid_h6/bias:0', 'spec_nn_dec/dec_nn_out_final/kernel:0', 'spec_nn_dec/dec_nn_out_final/bias:0']\n"
     ]
    }
   ],
   "source": [
    "# Define all trainable variable\n",
    "t_vars = tf.trainable_variables()\n",
    "\n",
    "# count model trainable variables\n",
    "print('[info] Total params: {}'.format(np.sum([np.prod(v.shape) for v in t_vars]).value))\n",
    "print('[info] Encoder params: {}'.format(np.sum([np.prod(v.shape) for v in t_vars if 'spec_nn_enc' in v.name]).value))\n",
    "print('[info] Decoder params: {}\\n'.format(np.sum([np.prod(v.shape) for v in t_vars if 'spec_nn_dec' in v.name]).value))\n",
    "\n",
    "# collect all tf variables\n",
    "nn_model_vars = [var for var in t_vars if 'spec_nn_enc' in var.name]\n",
    "nn_model_vars.extend([var for var in t_vars if 'spec_nn_dec' in var.name])\n",
    "\n",
    "# show all vars\n",
    "print([var.name for var in nn_model_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run testing loops here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Testing job runing...\n",
      "[info] 2019-03-05 11:59:00\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from ./darr_vae_model_ld_v033/darr_vae_model_ld_v033.ckpt\n",
      "[info] Model parameters are loaded.\n",
      "\n",
      "[info] Test [train data] start...\n",
      "[info] Train batch done: [ 10 / 176 ],  Note score: 98.8103 %\n",
      "[info] Train batch done: [ 20 / 176 ],  Note score: 99.7461 %\n",
      "[info] Train batch done: [ 30 / 176 ],  Note score: 99.8686 %\n",
      "[info] Train batch done: [ 40 / 176 ],  Note score: 99.5372 %\n",
      "[info] Train batch done: [ 50 / 176 ],  Note score: 98.7615 %\n",
      "[info] Train batch done: [ 60 / 176 ],  Note score: 99.2744 %\n",
      "[info] Train batch done: [ 70 / 176 ],  Note score: 99.2998 %\n",
      "[info] Train batch done: [ 80 / 176 ],  Note score: 99.5514 %\n",
      "[info] Train batch done: [ 90 / 176 ],  Note score: 99.9412 %\n",
      "[info] Train batch done: [ 100 / 176 ],  Note score: 99.6357 %\n",
      "[info] Train batch done: [ 110 / 176 ],  Note score: 99.3311 %\n",
      "[info] Train batch done: [ 120 / 176 ],  Note score: 98.7831 %\n",
      "[info] Train batch done: [ 130 / 176 ],  Note score: 99.4370 %\n",
      "[info] Train batch done: [ 140 / 176 ],  Note score: 99.3693 %\n",
      "[info] Train batch done: [ 150 / 176 ],  Note score: 98.2261 %\n",
      "[info] Train batch done: [ 160 / 176 ],  Note score: 98.8559 %\n",
      "[info] Train batch done: [ 170 / 176 ],  Note score: 97.3510 %\n",
      "[info] Train session is finished.\n",
      "\n",
      "\n",
      "[info] Test [test data] start...\n",
      "[info] test batch done: [ 10 / 177 ],  Note score: 98.8468 %\n",
      "[info] test batch done: [ 20 / 177 ],  Note score: 98.9903 %\n",
      "[info] test batch done: [ 30 / 177 ],  Note score: 98.9016 %\n",
      "[info] test batch done: [ 40 / 177 ],  Note score: 99.1461 %\n",
      "[info] test batch done: [ 50 / 177 ],  Note score: 97.8165 %\n",
      "[info] test batch done: [ 60 / 177 ],  Note score: 97.8201 %\n",
      "[info] test batch done: [ 70 / 177 ],  Note score: 99.2328 %\n",
      "[info] test batch done: [ 80 / 177 ],  Note score: 98.0989 %\n",
      "[info] test batch done: [ 90 / 177 ],  Note score: 97.7577 %\n",
      "[info] test batch done: [ 100 / 177 ],  Note score: 98.7188 %\n",
      "[info] test batch done: [ 110 / 177 ],  Note score: 97.9677 %\n",
      "[info] test batch done: [ 120 / 177 ],  Note score: 97.6993 %\n",
      "[info] test batch done: [ 130 / 177 ],  Note score: 98.2379 %\n",
      "[info] test batch done: [ 140 / 177 ],  Note score: 99.3444 %\n",
      "[info] test batch done: [ 150 / 177 ],  Note score: 97.1355 %\n",
      "[info] test batch done: [ 160 / 177 ],  Note score: 98.2706 %\n",
      "[info] test batch done: [ 170 / 177 ],  Note score: 98.7604 %\n",
      "[info] Test session is finished.\n",
      "\n",
      "\n",
      "[info] Total batch done: [ 353 ]\n",
      "[info] Train note score(Avg.): 99.1511 %\n",
      "[info] Train error notes per bar(736 notes): 6.2482\n",
      "[info] Test note score(Avg.): 98.4271 %\n",
      "[info] test error notes per bar(736 notes): 11.5766\n",
      "[info] Elapse Time: 0:00:41\n",
      "\n",
      "[info] Testing process is finished.\n",
      "[info] 2019-03-05 11:59:41\n"
     ]
    }
   ],
   "source": [
    "# Testing parameters setting\n",
    "chkpt_ver = chkpt_ver\n",
    "batch_size = 64\n",
    "show_info_epoch = 1\n",
    "show_info_batch = 10\n",
    "run_epoch_ratio_train = 1.0\n",
    "run_epoch_ratio_test = 1.0\n",
    "\n",
    "\n",
    "print (\"[info] Testing job runing...\")\n",
    "print ('[info] ' + datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') + '\\n')\n",
    "\n",
    "# init tensorflow variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(var_list=nn_model_vars)\n",
    "darr_vae_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "darr_vae_config.gpu_options.allow_growth = True\n",
    "\n",
    "# run TF session here\n",
    "with tf.Session(config=darr_vae_config) as sess:\n",
    "    \n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    # reload model\n",
    "    saver.restore(sess, './darr_vae_model_ld_{}/darr_vae_model_ld_{}.ckpt'.format(chkpt_ver, chkpt_ver))\n",
    "    print ('[info] Model parameters are loaded.\\n')\n",
    "    \n",
    "    epoch_target = 1\n",
    "    \n",
    "    # run Epoch loop here\n",
    "    for epoch_idx in range(0, epoch_target):                \n",
    "        \n",
    "        #################################################\n",
    "        #                                               #\n",
    "        #    run train data session starts from here    #  \n",
    "        #                                               #\n",
    "        #################################################\n",
    "              \n",
    "        print ('[info] Test [train data] start...')\n",
    "        \n",
    "        sess.run(train_iter.initializer)\n",
    "\n",
    "        note_score_train_list = []\n",
    "\n",
    "        session_bar_cqt_data_train_list = []\n",
    "        session_bar_note_num_train_list = []\n",
    "        session_bar_note_num_pred_train_list = []\n",
    "        session_bar_arrange_train_list = []\n",
    "        session_darr_output_train_list = []\n",
    "                \n",
    "        data_set_len_train = len(s100_index_code_list_train)\n",
    "        batch_target_train = int((data_set_len_train*run_epoch_ratio_train)//batch_size)\n",
    "        #batch_target_train = 1\n",
    "        \n",
    "        batch_runned_train = 0 \n",
    "        \n",
    "        # run batch loop here\n",
    "        for batch_idx in range(0, batch_target_train):            \n",
    "            \n",
    "            # run model testing            \n",
    "            session_bar_cqt_data_train,             \\\n",
    "            session_bar_style_data_train,           \\\n",
    "            session_bar_tempo_data_train,           \\\n",
    "            session_bar_note_num_train,             \\\n",
    "            session_bar_progress_train,             \\\n",
    "            session_bar_arrange_train,              \\\n",
    "            session_bar_note_num_pred_train,        \\\n",
    "            session_darr_output_train = sess.run([batch_bar_cqt_data_train,             \\\n",
    "                                                  batch_bar_style_data_train,           \\\n",
    "                                                  batch_bar_tempo_data_train,           \\\n",
    "                                                  batch_bar_note_num_train,             \\\n",
    "                                                  batch_bar_progress_train,             \\\n",
    "                                                  batch_bar_arrange_train,              \\\n",
    "                                                  vae_note_pred_train,                  \\\n",
    "                                                  vae_drum_out_train])\n",
    "            \n",
    "            # get correct shape of data\n",
    "            session_bar_cqt_data_train = session_bar_cqt_data_train.copy()[:,:,:,0]\n",
    "            session_bar_arrange_train = session_bar_arrange_train.copy()[:,:,:,0]\n",
    "            session_darr_output_train = session_darr_output_train.copy()[:,:,:,0]\n",
    "            \n",
    "            # calculate note score\n",
    "            bin_thv = 0.5\n",
    "            session_darr_output_train_bin = np.where(session_darr_output_train>=bin_thv,\n",
    "                                                     np.ones_like(session_darr_output_train),\n",
    "                                                     np.zeros_like(session_darr_output_train))  \n",
    "\n",
    "            note_score_train = 1.0 - np.sum(np.abs(  \\\n",
    "                session_bar_arrange_train - session_darr_output_train_bin))/np.prod(session_bar_arrange_train.shape)\n",
    "            \n",
    "            note_score_train_list.append(note_score_train)\n",
    "            \n",
    "            \n",
    "\n",
    "            # record every batch data\n",
    "            if len(session_bar_cqt_data_train_list)<200:\n",
    "                session_bar_cqt_data_train_list.append(session_bar_cqt_data_train)\n",
    "                session_bar_note_num_train_list.append(session_bar_note_num_train)\n",
    "                session_bar_note_num_pred_train_list.append(session_bar_note_num_pred_train)\n",
    "                session_bar_arrange_train_list.append(session_bar_arrange_train)\n",
    "                session_darr_output_train_list.append(session_darr_output_train)\n",
    "            \n",
    "            # record runned batch\n",
    "            batch_runned_train += 1\n",
    "            \n",
    "            if (batch_runned_train%show_info_batch)==0:\n",
    "                out_msg = \"[info] Train batch done: [ {} / {} ]\".format(batch_runned_train, batch_target_train)\n",
    "                out_msg += \",  Note score: {:.4f} %\".format(100*np.mean(note_score_train_list[-show_info_batch:]))\n",
    "                print (out_msg)        \n",
    "\n",
    "        # run train data session end from here                  \n",
    "        print ('[info] Train session is finished.\\n\\n')\n",
    "        \n",
    "        \n",
    "            \n",
    "        ################################################\n",
    "        #                                              #\n",
    "        #    run test data session starts from here    #  \n",
    "        #                                              #\n",
    "        ################################################\n",
    "\n",
    "        print ('[info] Test [test data] start...')\n",
    "        \n",
    "        sess.run(test_iter.initializer)\n",
    "\n",
    "        note_score_test_list = []\n",
    "\n",
    "        session_bar_cqt_data_test_list = []\n",
    "        session_bar_note_num_test_list = []\n",
    "        session_bar_note_num_pred_test_list = []\n",
    "        session_bar_arrange_test_list = []\n",
    "        session_darr_output_test_list = []\n",
    "        \n",
    "        session_zmn_list = []\n",
    "        session_zsd_list = []\n",
    "        session_zvalue_list = []\n",
    "        \n",
    "        data_set_len_test = len(s100_index_code_list_test)\n",
    "        batch_target_test = int((data_set_len_test*run_epoch_ratio_test)//batch_size)\n",
    "        \n",
    "        batch_runned_test = 0 \n",
    "        \n",
    "        # run batch loop here\n",
    "        for batch_idx in range(0, batch_target_test):            \n",
    "            \n",
    "            # run model testing            \n",
    "            session_bar_cqt_data_test,             \\\n",
    "            session_bar_style_data_test,           \\\n",
    "            session_bar_tempo_data_test,           \\\n",
    "            session_bar_note_num_test,             \\\n",
    "            session_bar_progress_test,             \\\n",
    "            session_bar_arrange_test,              \\\n",
    "            session_bar_note_num_pred_test,        \\\n",
    "            session_darr_output_test,              \\\n",
    "            session_zmn,                           \\\n",
    "            session_zsd,                           \\\n",
    "            session_zvalue = sess.run([batch_bar_cqt_data_test,            \\\n",
    "                                                 batch_bar_style_data_test,          \\\n",
    "                                                 batch_bar_tempo_data_test,          \\\n",
    "                                                 batch_bar_note_num_test,            \\\n",
    "                                                 batch_bar_progress_test,            \\\n",
    "                                                 batch_bar_arrange_test,             \\\n",
    "                                                 vae_note_pred_test,                 \\\n",
    "                                                 vae_drum_out_test, vae_latent_zmn_test, vae_latent_zsd_test, vae_latent_z_test])\n",
    "            \n",
    "            # get correct shape of data\n",
    "            session_bar_cqt_data_test = session_bar_cqt_data_test.copy()[:,:,:,0]\n",
    "            session_bar_arrange_test = session_bar_arrange_test.copy()[:,:,:,0]\n",
    "            session_darr_output_test = session_darr_output_test.copy()[:,:,:,0]\n",
    "            \n",
    "            # calculate note score\n",
    "            bin_thv = 0.5\n",
    "            session_darr_output_test_bin = np.where(session_darr_output_test>=bin_thv,\n",
    "                                                     np.ones_like(session_darr_output_test),\n",
    "                                                     np.zeros_like(session_darr_output_test))  \n",
    "\n",
    "            note_score_test = 1.0 - np.sum(np.abs(  \\\n",
    "                session_bar_arrange_test - session_darr_output_test_bin))/np.prod(session_bar_arrange_test.shape)\n",
    "            \n",
    "            note_score_test_list.append(note_score_test)\n",
    "            \n",
    "            \n",
    "\n",
    "            # record every batch data\n",
    "            if len(session_bar_cqt_data_test_list)<200:\n",
    "                session_bar_cqt_data_test_list.append(session_bar_cqt_data_test)\n",
    "                session_bar_note_num_test_list.append(session_bar_note_num_test)\n",
    "                session_bar_note_num_pred_test_list.append(session_bar_note_num_pred_test)\n",
    "                session_bar_arrange_test_list.append(session_bar_arrange_test)\n",
    "                session_darr_output_test_list.append(session_darr_output_test)\n",
    "                \n",
    "                session_zmn_list.append(session_zmn)\n",
    "                session_zsd_list.append(session_zsd)\n",
    "                session_zvalue_list.append(session_zvalue)\n",
    "                \n",
    "            \n",
    "            # record runned batch\n",
    "            batch_runned_test += 1\n",
    "            \n",
    "            if (batch_runned_test%show_info_batch)==0:\n",
    "                out_msg = \"[info] test batch done: [ {} / {} ]\".format(batch_runned_test, batch_target_test)\n",
    "                out_msg += \",  Note score: {:.4f} %\".format(100*np.mean(note_score_test_list[-show_info_batch:]))\n",
    "                print (out_msg)        \n",
    "\n",
    "        # test data session end            \n",
    "        print ('[info] Test session is finished.\\n')\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###############################\n",
    "        #                             #\n",
    "        #    show session info here   #  \n",
    "        #                             #\n",
    "        ###############################            \n",
    "            \n",
    "        delta_time = datetime.datetime.now() - start_time\n",
    "        \n",
    "        out_msg =  \"\\n[info] Total batch done: [ {} ]\".format(batch_target_train + batch_target_test)\n",
    "        out_msg += \"\\n[info] Train note score(Avg.): {:.4f} %\".format(100*np.mean(note_score_train_list))\n",
    "        out_msg += \"\\n[info] Train error notes per bar({} notes): {:.4f}\".format(dec_output_size, \n",
    "                                                                                (1-np.mean(note_score_train_list))*dec_output_size)\n",
    "        out_msg += \"\\n[info] Test note score(Avg.): {:.4f} %\".format(100*np.mean(note_score_test_list))\n",
    "        out_msg += \"\\n[info] test error notes per bar({} notes): {:.4f}\".format(dec_output_size, \n",
    "                                                                                (1-np.mean(note_score_test_list))*dec_output_size)\n",
    "        out_msg += \"\\n[info] Elapse Time: {}\".format(str(delta_time)[:-7])\n",
    "        \n",
    "        print (out_msg)\n",
    "        \n",
    "# show process is end\n",
    "print (\"\\n[info] Testing process is finished.\")\n",
    "print (\"[info] \" + datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check latent space z data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n",
      "(11328, 32)\n"
     ]
    }
   ],
   "source": [
    "print (len(session_zmn_list))\n",
    "#print (len(session_zsd_list))\n",
    "#print (len(session_zvalue_list))\n",
    "#print (session_zmn_list[0].shape)\n",
    "#print (session_zsd_list[0].shape)\n",
    "#print (session_zvalue_list[0].shape)\n",
    "\n",
    "session_zmn_ary =     np.concatenate(session_zmn_list, axis=0)\n",
    "session_zsd_ary =     np.concatenate(session_zsd_list, axis=0)\n",
    "session_zvu_ary =     np.concatenate(session_zvalue_list, axis=0)\n",
    "\n",
    "print (session_zmn_ary.shape)\n",
    "#print (session_zsd_ary.shape)\n",
    "#print (session_zvu_ary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save latent Z values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] No need to save file.\n"
     ]
    }
   ],
   "source": [
    "dump_file = 0\n",
    "\n",
    "latent_z_data_pkg = [session_zmn_ary,\n",
    "                     session_zsd_ary,\n",
    "                     session_zvu_ary]\n",
    "\n",
    "pkg_file_name = './large_dataset_data/model_test_result/s100_z_value_pkg.pkl'\n",
    "ensure_dir(pkg_file_name)\n",
    "\n",
    "if dump_file==1:\n",
    "    with open(pkg_file_name, 'wb') as pkl_file:\n",
    "        pickle.dump(latent_z_data_pkg, pkl_file)\n",
    "    \n",
    "    print ('[info] Z value pkl file is saved.')\n",
    "    \n",
    "else:\n",
    "    print ('[info] No need to save file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensure DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save s100 p00n/p03n/p06n/p12n/p20n test result here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] s100 v033, p20n test result saved.\n",
      "[info] saved file:  \"./large_dataset_data/model_test_result/v033/test_s100_p20n_result_pkg.pkl\"\n"
     ]
    }
   ],
   "source": [
    "test_result_train_pp = [session_bar_cqt_data_train_list,\n",
    "                        session_bar_note_num_train_list,\n",
    "                        session_bar_note_num_pred_train_list,\n",
    "                        session_bar_arrange_train_list,\n",
    "                        session_darr_output_train_list]\n",
    "\n",
    "test_result_test_pp = [session_bar_cqt_data_test_list,\n",
    "                       session_bar_note_num_test_list,\n",
    "                       session_bar_note_num_pred_test_list,\n",
    "                       session_bar_arrange_test_list,\n",
    "                       session_darr_output_test_list]\n",
    "\n",
    "test_s100_result_pkg = [test_result_train_pp, \n",
    "                        test_result_test_pp]\n",
    "\n",
    "\n",
    "test_s100_result_fname = './large_dataset_data/model_test_result/{}/rc_loss_{}/'.format(chkpt_ver, rc_loss_ver)\n",
    "test_s100_result_fname += 'test_s100_{}_result_pkg.pkl'.format(add_note_ver)\n",
    "\n",
    "ensure_dir(test_s100_result_fname)\n",
    "\n",
    "with open(test_s100_result_fname, 'wb') as pkl_file:\n",
    "    pickle.dump(test_s100_result_pkg, pkl_file)\n",
    "    \n",
    "print ('[info] s100 {}, {}, {} test result saved.'.format(chkpt_ver, rc_loss_ver, add_note_ver))\n",
    "print ('[info] saved file:  \\\"{}\\\"'.format(test_s100_result_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# restart kernel after run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require(\n",
       "    [\"base/js/dialog\"], \n",
       "    function(dialog) {\n",
       "        dialog.modal({\n",
       "                title: 'Notebook Halted',\n",
       "                body: 'This notebook is no longer running; the kernel has been halted. Close the browser tab, or, to continue working, restart the kernel.',\n",
       "                buttons: {\n",
       "                    'Kernel restart': { click: function(){ Jupyter.notebook.session.restart(); } }\n",
       "                }\n",
       "        });\n",
       "    }\n",
       ");\n",
       "Jupyter.notebook.session.delete();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "require(\n",
    "    [\"base/js/dialog\"], \n",
    "    function(dialog) {\n",
    "        dialog.modal({\n",
    "                title: 'Notebook Halted',\n",
    "                body: 'This notebook is no longer running; the kernel has been halted. Close the browser tab, or, to continue working, restart the kernel.',\n",
    "                buttons: {\n",
    "                    'Kernel restart': { click: function(){ Jupyter.notebook.session.restart(); } }\n",
    "                }\n",
    "        });\n",
    "    }\n",
    ");\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
